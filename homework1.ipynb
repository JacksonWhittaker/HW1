{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Basics of machine learning and feedforward neural networks\n",
    "\n",
    "### Name: [INPUT-YOUR NAME]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This assignment includes:\n",
    "\n",
    "## 1. Mathematical questions (20 points)\n",
    "\n",
    "## 2. Coding in Python (pytorch): train softmax classifiers on MNIST (80 points)\n",
    "\n",
    "2.0 Install and configure: python ([Anaconda platform](https://docs.anaconda.com/anaconda/install/) recommended), [Jupyter Notebook](https://jupyter.org/install) and [pytorch](https://pytorch.org/get-started/) \n",
    "\n",
    "2.1 Read provided code (with pytorch) \n",
    "\n",
    "2.2 Complete the code of mini-batch SGD for linear softmax classifier\n",
    "\n",
    "2.3 Record and plot results to show the loss and accuracy convergence (against #epoch)\n",
    "\n",
    "2.4 Complete the code of multilayer feedforward network and train the nonlinear models\n",
    "\n",
    "## Submission:\n",
    "\n",
    "* Convert the ipynb file to html file (**save the execution outputs**)\n",
    "    \n",
    "* Upload both your ipynb and html files to blackboard.\n",
    "\n",
    "* Deadline: Feb 28, 11:59:59 PM, Pacific time.\n",
    "\n",
    "## Hints for the coding part:\n",
    "\n",
    "1. If you are more comfortable to use other libraries such as Keras and TensorFlow, you can change the code accordingly.\n",
    "\n",
    "2. Implement your code using the computation of vectors and matrics, rather than the for-loop to compute each element in vectors and matrics.\n",
    "\n",
    "3. Plots should be clear and easy to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Mathematical questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Equivalence between: MLE using Bernoulli distribution and logistic regression\n",
    "\n",
    "Read Section 5.5, 6.2.2.2, 6.2.2.3 in [Deep Learning Book](https://www.deeplearningbook.org/). Explain why logistic regression is equivalent to Maximum likelihood estimation.\n",
    "\n",
    "Optional reading material that may help: [this lecture notes](https://www.stat.cmu.edu/~cshalizi/uADA/12/lectures/ch12.pdf), Section 12.2.1 and 12.2.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Determine the convexity of a function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.1. Suppose we have a functioin $f({\\bf x}) = \\log( \\sum_{i=1}^n \\exp(x_i) )$, where ${\\bf x} \\in \\mathbb R^d$ and $x_i$ is the $i$-th element of ${\\bf x}$ (so {\\bf x} is a vector and $x_i$ is a scalar). \n",
    "\n",
    "Question: is $f({\\bf x})$ a convex function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2.2. We introduced logistic regression for binary classification. Its objective function is \n",
    "\\begin{align*}\n",
    "f({\\bf w}) = \\frac{1}{n} \\sum_{i=1}^n \\log( 1 + \\exp( - y_i {\\bf w}^\\top {\\bf x}_i ) ) + \\frac{\\lambda}{2} \\| {\\bf w} \\|^2  ,\n",
    "\\end{align*}\n",
    "where ${\\bf x}_i \\in \\mathbb R^d, y_i \\in \\{ -1, +1 \\}$. \n",
    "\n",
    "Question: is the above objective function convex or not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "We can use the second order derivatives $\\nabla^2 f$ to see whether a function $f$ is convex: if the second order derivatives are positive definite (when $f$ has vector inputs), or positive number (when $f$ has scalar inputs). Suppose $f({\\bf x}) = 1/2 * {\\bf x}^\\top {\\bf x}$. Then we compute its first order and second order derivative:\n",
    "\\begin{align*}\n",
    "&\n",
    "\\nabla f({\\bf x}) = {\\bf x} \\\\\n",
    "&\n",
    "\\nabla^2 f({\\bf x}) = {\\bf 1} ,\n",
    "\\end{align*}\n",
    "where ${\\bf 1}$ is positive definite ([why?](https://math.stackexchange.com/questions/263957/why-is-the-identity-the-only-symmetric-0-1-matrix-with-all-eigenvalues-posit#:~:text=It%20does%20not%20give%20much,simultaneously%20has%20all%20three%20properties.&text=Symmetric%20and%20all%20eigenvalues%20positive,1's%20on%20its%20diagonal.)).\n",
    "\n",
    "Hint: you can find how to compute derivatives of vectors/matrics in Section 2 in [The Matrix Cookbook](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. (Read and run) Load MNIST dataset using pytorch and scale features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "train_set = datasets.MNIST('./data', train=True, download=True)\n",
    "test_set = datasets.MNIST('./data', train=False, download=True)\n",
    "\n",
    "x_train = train_set.data.numpy()\n",
    "x_train = x_train.reshape(len(x_train),-1)\n",
    "x_test = test_set.data.numpy()\n",
    "x_test = x_test.reshape(len(x_test),-1)\n",
    "\n",
    "y_train = train_set.targets.numpy()\n",
    "y_test = test_set.targets.numpy()\n",
    "\n",
    "print('Shape of x_train: ' + str(x_train.shape))\n",
    "print('Shape of x_test: ' + str(x_test.shape))\n",
    "\n",
    "print('Shape of y_train: ' + str(y_train.shape))\n",
    "print('Shape of y_test: ' + str(y_test.shape))\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. (Read and run) Visualize a random image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x_train.shape[0]\n",
    "random_index = numpy.random.randint(d, size=1)[0]\n",
    "plt.imshow(x_train[random_index].reshape(28,28))\n",
    "print('Its label is')\n",
    "print(y_train[random_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. (Read and run) Define softmax function and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"implement the softmax functions\n",
    "    input: numpy ndarray\n",
    "    output: numpy ndarray\n",
    "    \"\"\"\n",
    "    exp_list = numpy.exp(z)\n",
    "    result = 1/sum(exp_list) * exp_list\n",
    "    result = result.reshape((len(z),1))\n",
    "    assert (result.shape == (len(z),1))\n",
    "    return result\n",
    "\n",
    "def neg_log_loss(pred, label):\n",
    "    \"\"\"implement the negative log loss\"\"\"\n",
    "    loss = -numpy.log(pred[int(label)])\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 (Read and run) Functions for implementing linear softmax classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "root = os.path.abspath('.')\n",
    "root += '/configs/'\n",
    "\n",
    "def loadConfig(name):\n",
    "    \"\"\" Read a configuration file as a dictionary\"\"\"\n",
    "    full_path = root + name\n",
    "    json_file = open(full_path, 'r')\n",
    "    cfg = json.load(json_file)\n",
    "    json_file.close()\n",
    "    return cfg  \n",
    "\n",
    "def initialize(num_inputs,num_classes):\n",
    "    \"\"\"initialize the parameters\"\"\"\n",
    "    # num_inputs = 28*28 = 784\n",
    "    # num_classes = 10\n",
    "    w = numpy.zeros((num_classes, num_inputs)) # (10*784)\n",
    "    b = numpy.zeros((num_classes, 1)) # (10*1) \n",
    "    \n",
    "    param = {\n",
    "        'w' : w, # (10*784)\n",
    "        'b' : b  # (10*1)\n",
    "    }\n",
    "    return param\n",
    "\n",
    "def eval(param, hyp, x_data, y_data):\n",
    "    \"\"\" implement the evaluation function\n",
    "    input: param -- parameters dictionary (w, b)\n",
    "           hyp -- hyper-parameter: we use hyp['lambda'] to compute regularization\n",
    "           x_data -- x_train or x_test (size, 784)\n",
    "           y_data -- y_train or y_test (size,)\n",
    "    output: loss and accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    reg_lambda = hyp['lambda']\n",
    "    \n",
    "    # w: (10*784), x: (10000*784), y:(10000,)\n",
    "    loss_list = []\n",
    "    w = param['w'].transpose()\n",
    "    dist = numpy.array([numpy.squeeze(softmax(numpy.matmul(x_data[i], w))) for i in range(len(y_data))])\n",
    "\n",
    "    result = numpy.argmax(dist,axis=1)\n",
    "    accuracy = sum(result == y_data)/float(len(y_data))\n",
    "\n",
    "    loss_list = [neg_log_loss(dist[i],y_data[i]) for i in range(len(y_data))]\n",
    "    loss = sum(loss_list) / len(loss_list) + reg_lambda/2 * numpy.sum(w * w) + reg_lambda/2 * numpy.sum(b * b)\n",
    "    return loss, accuracy\n",
    "\n",
    "def train(param, hyp, x_train, y_train, x_test, y_test,cfg_idx):\n",
    "    \"\"\" implement the train function\n",
    "    input: param -- parameters dictionary (w, b)\n",
    "           hyp -- hyperparameters dictionary\n",
    "           x_train -- (60000, 784)\n",
    "           y_train -- (60000,)\n",
    "           x_test -- x_test (10000, 784)\n",
    "           y_test -- y_test (10000,)\n",
    "    output: train_loss_list, train_acc_list, test_loss_list, test_acc_list\n",
    "           Four lists contain the epoch-wise loss function on training data, accuracy on training data, loss function on testing data, accuracy on testing data, respectively\n",
    "    \"\"\"\n",
    "    num_epoches = hyp['num_epoches']\n",
    "    batch_size = hyp['batch_size']\n",
    "    learning_rate = hyp['learning_rate']\n",
    "    mu = hyp['mu']\n",
    "    reg_lambda = hyp['lambda']\n",
    "    train_loss_list, train_acc_list, test_loss_list, test_acc_list = [],[],[],[]\n",
    "    if bool(hyp['momentum']) == True:\n",
    "        w_velocity = numpy.zeros(param['w'].shape)\n",
    "        b_velocity = numpy.zeros(param['b'].shape) \n",
    "\n",
    "    for epoch in range(num_epoches):\n",
    "        \n",
    "        # select the random sequence of training set\n",
    "        rand_indices = numpy.random.choice(x_train.shape[0],x_train.shape[0],replace=False)\n",
    "        num_batch = int(x_train.shape[0]/batch_size)\n",
    "        \n",
    "        if bool(hyp['learning_decay']) == True:\n",
    "            try:\n",
    "                if test_acc_list[-1] - test_acc_list[-2] < 0.001:\n",
    "                    learning_rate *= hyp['decay_factor']\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            message = 'learning rate: %.8f' % learning_rate\n",
    "            print(message)\n",
    "            logging.info(message)\n",
    "\n",
    "        # for each batch of train data\n",
    "        for batch in range(num_batch):\n",
    "            index = rand_indices[batch_size*batch:batch_size*(batch+1)]\n",
    "            x_batch = x_train[index]\n",
    "            y_batch = y_train[index]\n",
    "\n",
    "            # calculate the stochastic gradient w.r.t w and b\n",
    "            dw, db, batch_loss = mini_batch_gradient(param, x_batch, y_batch, reg_lambda)\n",
    "\n",
    "            param['w'] -= learning_rate * dw\n",
    "            param['b'] -= learning_rate * db\n",
    "            \n",
    "            if (batch+1) % 100 == 0:\n",
    "                message = 'Epoch [%d/%d], Batch [%d/%d], Loss %.4f' % (epoch+1, num_epoches, batch+1, num_batch, batch_loss)\n",
    "                print(message)\n",
    "\n",
    "        train_loss, train_acc = eval(param,hyp,x_train,y_train)\n",
    "        test_loss, test_acc = eval(param,hyp,x_test,y_test)\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_loss_list.append(test_loss)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        message = 'Epoch %d/%d, Train Loss %.4f, Train Acc %.4f, Test Loss %.4f, Test Acc %.4f' % (epoch+1, num_epoches, train_loss, train_acc, test_loss, test_acc)\n",
    "        print(message)\n",
    "        logging.info(message)\n",
    "    return train_loss_list, train_acc_list, test_loss_list, test_acc_list\n",
    "\n",
    "\n",
    "def plot(train_loss_list, train_acc_list, test_loss_list, test_acc_list, cfg_idx):\n",
    "    \"\"\"store the plots\"\"\"\n",
    "    # epoch_list = list(range(len(loss_list)))\n",
    "    plt.plot(train_loss_list, '-b', label='train loss')\n",
    "    plt.plot(test_loss_list, '-r', label='test loss')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Loss Function')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title('Loss Function ~ Epoch')\n",
    "    plt.savefig('assets/loss_{}.png'.format(cfg_idx))\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(train_acc_list, '-b', label='train acc')\n",
    "    plt.plot(test_acc_list, '-r', label='test acc')\n",
    "    plt.legend()\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.xticks(rotation=60)\n",
    "    plt.title('Accuracy ~ Epoch')\n",
    "    plt.savefig('assets/accr_{}.png'.format(cfg_idx))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def main(cfg_idx): \n",
    "#     cfg_idx = args.config\n",
    "    cfg_name = 'config_{}.json'.format(cfg_idx)\n",
    "    hyperpara = loadConfig(cfg_name)\n",
    "\n",
    "    # setting the random seed\n",
    "    numpy.random.seed(1024)\n",
    "\n",
    "    # initialize the parameters\n",
    "    num_inputs = x_train.shape[1]\n",
    "    num_classes = len(set(y_train))\n",
    "    param = initialize(num_inputs,num_classes)\n",
    "\n",
    "    # train the model\n",
    "    train_loss_list, train_acc_list, test_loss_list, test_acc_list = train(param,hyperpara,x_train,y_train,x_test,y_test, cfg_idx)\n",
    "\n",
    "    # plot the loss and accuracy\n",
    "    plot(train_loss_list, train_acc_list, test_loss_list, test_acc_list, cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5. (To finish) Implementation of mini-batch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_gradient(param, x_batch, y_batch, reg_lambda):\n",
    "    \"\"\"implement the function to compute the mini batch gradient\n",
    "    input: param -- parameters dictionary (w, b)\n",
    "           x_batch -- a batch of x (size, 784)\n",
    "           y_batch -- a batch of y (size,)\n",
    "           reg_lambdba -- regularization parameter\n",
    "    output: \n",
    "           dw -- derivative for weight w\n",
    "           db -- derivative for bias b\n",
    "           batch_loss -- average loss on the mini-batch samples\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code goes here\n",
    "\n",
    "\n",
    "    \n",
    "    return dw, db, batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6. (Read and run) Train your model using the provided configuration (in *configs/config_sample.json*) untill convergence "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    cfg_idx = 'sample'\n",
    "    \n",
    "    import logging\n",
    "    logging.basicConfig(filename=\"./logs/{}.log\".format(cfg_idx), filemode=\"w\", format=\"%(message)s\", level=logging.DEBUG)\n",
    "    \n",
    "    main(cfg_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7. (To finish) Does $\\lambda$ impact the accuracy on training and testing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please use different values for $\\lambda \\in \\{ 0, 0.0001, 0.001, 0.01, 0.1, 1 \\}$ in the above linear classifier.\n",
    "For the six values of $\\lambda$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Plot loss function values on training and testing data.\n",
    "\n",
    "(2) Plot accuracy on training data and testing data.\n",
    "\n",
    "(3) Use these plots to conclude whether regularization may help generalization performance, and explain why you can draw this conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8. (Read and run) Train feedforward networks with one hidden layer (one activation layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "input_size = 784\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "# MNIST dataset \n",
    "train_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),  \n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data', \n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.00001)  \n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9. (To finish) Implement and train a feedforward network with two hidden layers (two activation layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training: plot training and testing accuracy (against #epoches) and answer the question: is there performance difference between one-layer and two-layer network?\n",
    "\n",
    "Hint: modify class NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n",
    "# modify class NeuralNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *1* the training part of code can keep unchanged\n",
    "\n",
    "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "    # Test the model\n",
    "    # In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "\n",
    "        \n",
    "# *2* Code for plotting results\n",
    "### Your code goes here\n",
    "###     \n",
    "        \n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10. (To finish) Use SGD (instead of Adam) to train your two-hidden-layer network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: read [this document](https://pytorch.org/docs/stable/optim.html) for torch.optim to understand how to change optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.11. (To finish) Use SGD to train your two-hidden-layer network with different learning rate values in the range of $\\{ 0, 0.0001, 0.001, 0.01, 0.1, 1 \\}$, and show which learning rate achieves the best testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.12. (To finish) Use Adam to train your two-hidden-layer network with different learning rate values in the range of $\\{ 0, 0.0001, 0.001, 0.01, 0.1, 1 \\}$, and show which learning rate achieves the best testing accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13. (To finish) Change the dimension of the hidden variable (*hidden_size*) from $500$ to $100, 1000, 2000$, train the corresponding networks, and show the difference of them in testing accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question: how the dimension of hidden variable impacts the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.13. (To finish) Is the best learning rate for SGD the same with the best learning rate for Adam?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
